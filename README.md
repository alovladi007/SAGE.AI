# Academic Integrity Platform - Complete Documentation

## üìã Table of Contents
1. [Project Overview](#project-overview)
2. [Architecture](#architecture)
3. [Features](#features)
4. [Installation](#installation)
5. [Configuration](#configuration)
6. [Deployment](#deployment)
7. [API Documentation](#api-documentation)
8. [ML Models](#ml-models)
9. [Monitoring](#monitoring)
10. [Security](#security)
11. [Performance](#performance)
12. [Troubleshooting](#troubleshooting)

## üéØ Project Overview

The Academic Integrity Platform is a comprehensive ML-powered system designed to detect plagiarism, data fabrication, and other forms of academic misconduct across millions of research papers and theses.

### Key Capabilities
- **Multi-modal plagiarism detection** (text, images, data)
- **Statistical anomaly detection** (p-hacking, GRIM test, Benford's Law)
- **Citation network analysis**
- **Real-time processing** with batch capabilities
- **Scalable to millions of documents**
- **99.9% uptime SLA**

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Load Balancer (Nginx)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Frontend (React)  ‚îÇ         ‚îÇ   Backend API       ‚îÇ
    ‚îÇ   - Dashboard       ‚îÇ         ‚îÇ   - FastAPI         ‚îÇ
    ‚îÇ   - Upload UI       ‚îÇ         ‚îÇ   - WebSocket       ‚îÇ
    ‚îÇ   - Reports         ‚îÇ         ‚îÇ   - REST Endpoints  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ                             ‚îÇ                     ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   ML Pipeline        ‚îÇ    ‚îÇ   Batch Processor      ‚îÇ  ‚îÇ  Monitoring ‚îÇ
    ‚îÇ   - SciBERT          ‚îÇ    ‚îÇ   - Celery Workers     ‚îÇ  ‚îÇ  - Prometheus‚îÇ
    ‚îÇ   - SPECTER2         ‚îÇ    ‚îÇ   - Ray Cluster        ‚îÇ  ‚îÇ  - Grafana  ‚îÇ
    ‚îÇ   - Custom Models    ‚îÇ    ‚îÇ   - Scheduled Jobs     ‚îÇ  ‚îÇ  - Alerts   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ                              ‚îÇ                     ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                          Data Layer                                  ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
    ‚îÇ  ‚îÇ PostgreSQL ‚îÇ  ‚îÇ Elasticsearch ‚îÇ  ‚îÇ  Redis  ‚îÇ  ‚îÇ   MinIO    ‚îÇ    ‚îÇ
    ‚îÇ  ‚îÇ  (Primary) ‚îÇ  ‚îÇ (Full-text)   ‚îÇ  ‚îÇ (Cache) ‚îÇ  ‚îÇ  (Storage) ‚îÇ    ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## ‚ú® Features

### Core Detection Capabilities

#### 1. Text Similarity Detection
- **Verbatim matching** with dynamic programming
- **Semantic similarity** using transformer embeddings
- **Paraphrase detection** with sentence-level analysis
- **Structural similarity** analysis

#### 2. Statistical Anomaly Detection
- **P-value validation** and p-hacking detection
- **GRIM test** for mean/sample size consistency
- **Benford's Law** analysis
- **Confidence interval validation**

#### 3. Image Analysis
- **Duplicate detection** using perceptual hashing
- **Manipulation detection** with SSIM and feature matching
- **Figure reuse tracking** across papers

#### 4. Citation Analysis
- **Self-citation ratio** monitoring
- **Citation ring detection**
- **Predatory journal identification**
- **Reference age analysis**

### Platform Features

- **Real-time processing** with WebSocket updates
- **Batch processing** for large-scale analysis
- **Comprehensive reporting** with PDF export
- **Multi-institution support**
- **API access** for integration
- **Role-based access control**
- **Audit logging**

## üì¶ Installation

### Prerequisites

- Python 3.11+
- Node.js 18+
- Docker & Docker Compose
- PostgreSQL 15+
- Redis 7+
- CUDA 11.8+ (for GPU acceleration)

### Requirements Files

#### backend/requirements.txt
```txt
# Core dependencies
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6

# Database
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
alembic==1.12.1

# ML and NLP
torch==2.1.0
transformers==4.35.0
sentence-transformers==2.2.2
scikit-learn==1.3.2
numpy==1.24.3
pandas==2.1.3
scipy==1.11.4
spacy==3.7.2

# PDF processing
PyPDF2==3.0.1
pdfplumber==0.10.3
PyMuPDF==1.23.7
camelot-py==0.11.0

# Image processing
opencv-python==4.8.1
Pillow==10.1.0
scikit-image==0.22.0
imagehash==4.3.1

# Task queue
celery[redis]==5.3.4
redis==5.0.1
ray==2.8.0

# Storage
minio==7.2.0
boto3==1.29.7

# Search
elasticsearch==8.11.0

# Monitoring
prometheus-client==0.19.0
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0

# API and networking
httpx==0.25.1
websockets==12.0
aioredis==2.0.1
aiofiles==23.2.1

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0

# Utilities
python-dotenv==1.0.0
pyyaml==6.0.1
jsonschema==4.20.0
```

#### ml_worker/requirements-ml.txt
```txt
# ML specific dependencies
torch==2.1.0+cu118
torchvision==0.16.0+cu118
torchaudio==2.1.0+cu118
--extra-index-url https://download.pytorch.org/whl/cu118

# Advanced ML
faiss-gpu==1.7.4
lightgbm==4.1.0
xgboost==2.0.2
catboost==1.2.2

# NLP models
transformers==4.35.0
sentence-transformers==2.2.2
spacy[cuda118]==3.7.2
nltk==3.8.1

# Computer vision
detectron2==0.6
albumentations==1.3.1
timm==0.9.10

# Scientific computing
numba==0.58.1
cupy-cuda11x==12.3.0
rapids-cudf==23.12.0

# OCR
pytesseract==0.3.10
easyocr==1.7.1
```

#### frontend/package.json
```json
{
  "name": "academic-integrity-frontend",
  "version": "1.0.0",
  "private": true,
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-router-dom": "^6.20.0",
    "axios": "^1.6.2",
    "lucide-react": "^0.294.0",
    "recharts": "^2.10.1",
    "d3": "^7.8.5",
    "tailwindcss": "^3.3.6",
    "date-fns": "^2.30.0",
    "react-query": "^3.39.3",
    "socket.io-client": "^4.5.4",
    "react-hook-form": "^7.48.2",
    "react-dropzone": "^14.2.3",
    "react-pdf": "^7.5.1",
    "@tanstack/react-table": "^8.10.7",
    "zustand": "^4.4.7"
  },
  "devDependencies": {
    "@types/react": "^18.2.42",
    "@types/react-dom": "^18.2.17",
    "@vitejs/plugin-react": "^4.2.0",
    "vite": "^5.0.6",
    "typescript": "^5.3.2",
    "eslint": "^8.55.0",
    "prettier": "^3.1.0"
  },
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview",
    "test": "vitest",
    "lint": "eslint src --ext ts,tsx --report-unused-disable-directives --max-warnings 0"
  }
}
```

### Quick Start

```bash
# Clone the repository
git clone https://github.com/your-org/academic-integrity-platform.git
cd academic-integrity-platform

# Create environment file
cp .env.example .env
# Edit .env with your configuration

# Start with Docker Compose
docker-compose up -d

# Initialize database
docker-compose exec backend python scripts/init_db.py

# Create admin user
docker-compose exec backend python scripts/create_admin.py

# Access the platform
# Frontend: http://localhost:3000
# API: http://localhost:8000
# Grafana: http://localhost:3001
```

## ‚öôÔ∏è Configuration

### Environment Variables (.env)
```env
# Database
DB_HOST=postgres
DB_PORT=5432
DB_NAME=academic_integrity
DB_USER=aiplatform
DB_PASSWORD=secure_password_here

# Redis
REDIS_URL=redis://redis:6379

# Elasticsearch
ELASTICSEARCH_URL=http://elasticsearch:9200

# MinIO
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123

# Security
JWT_SECRET=your_jwt_secret_here
SECRET_KEY=your_secret_key_here

# ML Models
MODEL_CACHE_DIR=/models
CUDA_VISIBLE_DEVICES=0,1

# Monitoring
PROMETHEUS_PORT=9090
GRAFANA_USER=admin
GRAFANA_PASSWORD=admin

# Email (for alerts)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=alerts@yourdomain.com
SMTP_PASSWORD=your_password

# API Keys (optional)
OPENAI_API_KEY=sk-...
HUGGING_FACE_TOKEN=hf_...
```

## üöÄ Deployment

### Production Deployment with Kubernetes

```bash
# Build and push Docker images
docker build -t your-registry/academic-integrity-backend:latest ./backend
docker build -t your-registry/academic-integrity-frontend:latest ./frontend
docker build -t your-registry/academic-integrity-ml-worker:latest ./ml_worker

docker push your-registry/academic-integrity-backend:latest
docker push your-registry/academic-integrity-frontend:latest
docker push your-registry/academic-integrity-ml-worker:latest

# Deploy to Kubernetes
kubectl apply -f kubernetes/namespace.yaml
kubectl apply -f kubernetes/secrets.yaml
kubectl apply -f kubernetes/configmap.yaml
kubectl apply -f kubernetes/deployments/
kubectl apply -f kubernetes/services/
kubectl apply -f kubernetes/ingress.yaml

# Scale workers
kubectl scale deployment ml-worker -n academic-integrity --replicas=10
```

### Production Checklist

- [ ] SSL/TLS certificates configured
- [ ] Database backups scheduled
- [ ] Monitoring and alerting configured
- [ ] Rate limiting enabled
- [ ] CDN configured for static assets
- [ ] WAF rules configured
- [ ] Log aggregation setup
- [ ] Disaster recovery plan tested
- [ ] Performance testing completed
- [ ] Security audit performed

## üìö API Documentation

### Authentication
```bash
# Get access token
curl -X POST http://api.example.com/auth/token \
  -H "Content-Type: application/json" \
  -d '{"username": "user@example.com", "password": "password"}'

# Use token in requests
curl -H "Authorization: Bearer YOUR_TOKEN" \
  http://api.example.com/api/papers
```

### Key Endpoints

#### Upload Paper
```http
POST /api/papers/upload
Content-Type: multipart/form-data

file: (binary)
metadata: {
  "title": "Paper Title",
  "authors": ["Author 1", "Author 2"],
  "journal": "Journal Name"
}
```

#### Get Analysis Results
```http
GET /api/papers/{paper_id}/analyze
Authorization: Bearer TOKEN

Response:
{
  "paper_id": "uuid",
  "overall_risk_score": 0.75,
  "similarity_findings": [...],
  "anomaly_findings": [...],
  "recommendations": [...]
}
```

#### Batch Processing
```http
POST /api/batch/process
Authorization: Bearer TOKEN
Content-Type: application/json

{
  "paper_ids": ["id1", "id2", "id3"],
  "job_type": "full_analysis",
  "priority": "high"
}
```

## ü§ñ ML Models

### Pre-trained Models Used
1. **SciBERT** - Scientific text understanding
2. **SPECTER2** - Academic paper embeddings
3. **LayoutLM** - Document layout analysis
4. **EfficientNet** - Image similarity detection

### Custom Models
- **Citation Network GNN** - Graph neural network for citation analysis
- **Anomaly Detection Ensemble** - Custom ensemble for statistical anomalies
- **Style Transfer Detector** - Detecting paraphrasing patterns

### Model Training
```python
# Training custom models
python ml_worker/train_custom_model.py \
  --model-type anomaly_detector \
  --dataset path/to/dataset \
  --epochs 100 \
  --batch-size 32 \
  --learning-rate 0.001
```

## üìä Monitoring

### Metrics Tracked
- **System Metrics**: CPU, Memory, Disk, Network
- **Application Metrics**: Request rate, Error rate, Latency
- **Business Metrics**: Papers processed, Anomalies detected, Risk scores

### Grafana Dashboards
1. **System Overview** - Overall platform health
2. **Processing Pipeline** - ML pipeline performance
3. **Alert Dashboard** - Active alerts and incidents
4. **Business Intelligence** - Usage statistics and trends

### Alert Rules
- High error rate (>5%)
- Processing queue backup (>500 papers)
- High risk papers spike (>10 in 1 hour)
- System resource exhaustion
- Database connection pool exhaustion

## üîí Security

### Security Features
- **End-to-end encryption** for sensitive data
- **Role-based access control** (RBAC)
- **API rate limiting** and throttling
- **SQL injection prevention**
- **XSS protection**
- **CSRF tokens**
- **Audit logging**
- **Data anonymization** options

### Compliance
- GDPR compliant
- FERPA compliant
- ISO 27001 aligned
- SOC 2 Type II ready

## ‚ö° Performance

### Benchmarks
- **Upload Speed**: 10MB PDF in <2 seconds
- **Processing Time**: Average 2.3 minutes per paper
- **Similarity Search**: <100ms for corpus of 1M papers
- **API Response Time**: p99 <200ms
- **Concurrent Users**: 10,000+
- **Papers/Day**: 50,000+

### Optimization Tips
1. **Enable GPU acceleration** for ML models
2. **Use Redis caching** aggressively
3. **Implement database connection pooling**
4. **Use CDN for static assets**
5. **Enable gzip compression**
6. **Optimize database indexes**
7. **Use async processing where possible**

## üîß Troubleshooting

### Common Issues

#### High Memory Usage
```bash
# Check memory usage
docker stats

# Increase memory limits
docker-compose down
# Edit docker-compose.yml memory limits
docker-compose up -d
```

#### Slow Processing
```bash
# Check worker status
celery -A celery_app inspect active

# Scale workers
docker-compose scale ml_worker=5
```

#### Database Connection Issues
```bash
# Check database connectivity
docker-compose exec backend python -c "from sqlalchemy import create_engine; engine = create_engine('postgresql://...'); engine.connect()"

# Reset connections
docker-compose restart backend
```

### Logs
```bash
# View logs
docker-compose logs -f backend
docker-compose logs -f ml_worker

# Export logs
docker-compose logs > logs.txt
```

## üìà Roadmap

### Q1 2025
- [ ] Multi-language support (Chinese, Spanish, Arabic)
- [ ] Mobile application
- [ ] Advanced visualization dashboard
- [ ] Integration with major journals

### Q2 2025
- [ ] Blockchain-based verification
- [ ] Real-time collaboration features
- [ ] Advanced AI explainability
- [ ] Automated report generation

### Q3 2025
- [ ] Federated learning implementation
- [ ] Cross-institutional data sharing
- [ ] Patent analysis capabilities
- [ ] Grant proposal checking

## ü§ù Contributing

Please see [CONTRIBUTING.md](CONTRIBUTING.md) for development guidelines.

## üìÑ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file.

## üí¨ Support

- Documentation: https://docs.academic-integrity.com
- Email: support@academic-integrity.com
- Slack: https://academic-integrity.slack.com
- Issues: https://github.com/your-org/academic-integrity-platform/issues

---

Built with ‚ù§Ô∏è for academic integrity
